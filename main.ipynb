{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "from model.decoder import Decoder\n",
    "from model.encoder import CNN_Encoder\n",
    "from model.attention import BahdanauAttention\n",
    "from model.decoder import embedding_initializer\n",
    "from components.positional import add_timing_signal_nd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(match_file, images_dir, formulas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        match_file: path of file where the file containing matches live. \n",
    "            match_file can take values {test.matching.txt, train.matching.txt, val.matching.txt}\n",
    "        images_dir: directory where the images live. \n",
    "            Can take the following values only: ./data/ + {images_test, images_train, val_train}\n",
    "        formulas: arr containg formulas.\n",
    "            Formulas should come from one of the following files: {test.formulas.norm.txt, train.formulas.norm.txt, val.formulas.norm.txt}\n",
    "            \n",
    "    Return:\n",
    "        matching_formulas: arr of formulas that matches the imgs. \n",
    "            Must be same length as matching_images_paths. Each index corresponds to an img in matchign_images_paths at equal index\n",
    "        matching_images_paths: arr of imgs that match formulas.\n",
    "            Must be same length as matching_formulas. Each index corresponds to formula in matching_formula at equal index\n",
    "    \n",
    "    \"\"\"\n",
    "    matching_formulas = []\n",
    "    matching_images_paths = []\n",
    "    \n",
    "    matches = open(match_file).read().split('\\n')\n",
    "    \n",
    "    for match in matches:\n",
    "        # check if empty since last line is always empty\n",
    "        if len(match) != 0:\n",
    "            # tuple with form (img file name, formula line)\n",
    "            match_tuple = match.split(' ')\n",
    "            # file name\n",
    "            img_name = match_tuple[0]\n",
    "            # line number in formula_file\n",
    "            idx = int(match_tuple[1])\n",
    "\n",
    "            # get the image path\n",
    "            img_path = os.fspath(os.path.join(os.curdir, images_dir, img_name))\n",
    "            # add start and end tokens\n",
    "            formula = \"<start> \" + formulas[idx] + \" <end>\"\n",
    "\n",
    "            matching_images_paths.append(img_path)\n",
    "            matching_formulas.append(formula)\n",
    "        \n",
    "    return matching_images_paths, matching_formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_formulas = open(\"data/test.formulas.norm.txt\").read().split('\\n')\n",
    "all_train_formulas = open(\"data/train.formulas.norm.txt\").read().split('\\n')\n",
    "all_val_formulas = open(\"data/val.formulas.norm.txt\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_formulas = get_matches(\"data/test.matching.txt\", \"data/images_test\", all_test_formulas)\n",
    "train_images, train_formulas = get_matches(\"data/train.matching.txt\", \"data/images_train\", all_train_formulas)\n",
    "val_images, val_formulas = get_matches(\"data/val.matching.txt\", \"data/images_val\", all_val_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9442, 9442)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images), len(test_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76304, 76304)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(train_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8474, 8474)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_images), len(val_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_formulas = test_formulas + train_formulas + val_formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 400\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='')\n",
    "tokenizer.fit_on_texts(all_formulas)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formula_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 80\n",
    "BATCH_SIZE = 20\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, formula):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img)\n",
    "    img = tf.image.resize_with_pad(img, 300, 50)\n",
    "    return img, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_only(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize_with_pad(img, 300, 50)\n",
    "    img = img / 255\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_formula_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(embedding_dim, 512, vocab_size=top_k+1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img, target):\n",
    "    loss = 0\n",
    "    img = img / 255\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "            \n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "        \n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        return loss, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restore to latest cehckpoint\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-25-c37b197b996d>:10 train_step  *\n        features = encoder(img)\n    /Users/frankpalma/Projects/img2latex/model/encoder.py:52 call  *\n        x = self.dense(x)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:982 __call__  **\n        self._maybe_build(inputs)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2643 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:1171 build\n        self.kernel = self.add_weight(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:597 add_weight\n        variable = self._add_variable_with_custom_getter(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:730 _add_variable_with_custom_getter\n        checkpoint_initializer = self._preload_simple_restoration(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:797 _preload_simple_restoration\n        return CheckpointInitialValue(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:75 __init__\n        self.wrapped_value.set_shape(shape)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1207 set_shape\n        raise ValueError(\n\n    ValueError: Tensor's shape (12288, 80) is not compatible with supplied shape [17920, 80]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a751a4bc744e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 696\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    697\u001b[0m             *args, **kwds))\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-25-c37b197b996d>:10 train_step  *\n        features = encoder(img)\n    /Users/frankpalma/Projects/img2latex/model/encoder.py:52 call  *\n        x = self.dense(x)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:982 __call__  **\n        self._maybe_build(inputs)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2643 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:1171 build\n        self.kernel = self.add_weight(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:597 add_weight\n        variable = self._add_variable_with_custom_getter(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:730 _add_variable_with_custom_getter\n        checkpoint_initializer = self._preload_simple_restoration(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:797 _preload_simple_restoration\n        return CheckpointInitialValue(\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:75 __init__\n        self.wrapped_value.set_shape(shape)\n    /Users/frankpalma/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1207 set_shape\n        raise ValueError(\n\n    ValueError: Tensor's shape (12288, 80) is not compatible with supplied shape [17920, 80]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img, target)\n",
    "        total_loss += t_loss\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "        \n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "        \n",
    "    img = tf.expand_dims(load_image_only(image), 0) \n",
    "    \n",
    "    features = encoder(img)\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        \n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Formula:', real_caption)\n",
    "print ('Prediction Formula:', ' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset:\n",
    "    print(img.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
