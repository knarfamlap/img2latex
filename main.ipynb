{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "from model.decoder import Decoder\n",
    "from model.encoder import CNN_Encoder\n",
    "from model.attention import BahdanauAttention\n",
    "from model.decoder import embedding_initializer\n",
    "from components.positional import add_timing_signal_nd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(match_file, images_dir, formulas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        match_file: path of file where the file containing matches live. \n",
    "            match_file can take values {test.matching.txt, train.matching.txt, val.matching.txt}\n",
    "        images_dir: directory where the images live. \n",
    "            Can take the following values only: ./data/ + {images_test, images_train, val_train}\n",
    "        formulas: arr containg formulas.\n",
    "            Formulas should come from one of the following files: {test.formulas.norm.txt, train.formulas.norm.txt, val.formulas.norm.txt}\n",
    "            \n",
    "    Return:\n",
    "        matching_formulas: arr of formulas that matches the imgs. \n",
    "            Must be same length as matching_images_paths. Each index corresponds to an img in matchign_images_paths at equal index\n",
    "        matching_images_paths: arr of imgs that match formulas.\n",
    "            Must be same length as matching_formulas. Each index corresponds to formula in matching_formula at equal index\n",
    "    \n",
    "    \"\"\"\n",
    "    matching_formulas = []\n",
    "    matching_images_paths = []\n",
    "    \n",
    "    matches = open(match_file).read().split('\\n')\n",
    "    \n",
    "    for match in matches:\n",
    "        # check if empty since last line is always empty\n",
    "        if len(match) != 0:\n",
    "            # tuple with form (img file name, formula line)\n",
    "            match_tuple = match.split(' ')\n",
    "            # file name\n",
    "            img_name = match_tuple[0]\n",
    "            # line number in formula_file\n",
    "            idx = int(match_tuple[1])\n",
    "\n",
    "            # get the image path\n",
    "            img_path = os.fspath(os.path.join(os.curdir, images_dir, img_name))\n",
    "            # add start and end tokens\n",
    "            formula = \"<start> \" + formulas[idx] + \" <end>\"\n",
    "\n",
    "            matching_images_paths.append(img_path)\n",
    "            matching_formulas.append(formula)\n",
    "        \n",
    "    return matching_images_paths, matching_formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_formulas = open(\"data/test.formulas.norm.txt\").read().split('\\n')\n",
    "all_train_formulas = open(\"data/train.formulas.norm.txt\").read().split('\\n')\n",
    "all_val_formulas = open(\"data/val.formulas.norm.txt\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_formulas = get_matches(\"data/test.matching.txt\", \"data/images_test\", all_test_formulas)\n",
    "train_images, train_formulas = get_matches(\"data/train.matching.txt\", \"data/images_train\", all_train_formulas)\n",
    "val_images, val_formulas = get_matches(\"data/val.matching.txt\", \"data/images_val\", all_val_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9442, 9442)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images), len(test_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76304, 76304)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(train_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8474, 8474)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_images), len(val_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_formulas = test_formulas + train_formulas + val_formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 400\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='')\n",
    "tokenizer.fit_on_texts(all_formulas)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formula_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 80\n",
    "BATCH_SIZE = 20\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, formula):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img)\n",
    "    img = tf.image.resize_with_pad(img, 50, 300)\n",
    "    img = img / 255\n",
    "    return img, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_only(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize_with_pad(img, 50, 300)\n",
    "    img = img / 255\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_formula_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(embedding_dim, 512, vocab_size=top_k+1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "            \n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "        \n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        return loss, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restore to latest cehckpoint\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 Batch 0 Loss 2.7063 \n",
      " 100/3816 [..............................] - ETA: 1:51:35 Epoch 1 Batch 100 Loss 1.3674 \n",
      " 200/3816 [>.............................] - ETA: 1:24:20 Epoch 1 Batch 200 Loss 1.5530 \n",
      " 300/3816 [=>............................] - ETA: 1:14:56 Epoch 1 Batch 300 Loss 1.5211 \n",
      " 400/3816 [==>...........................] - ETA: 1:08:57 Epoch 1 Batch 400 Loss 1.3599 \n",
      " 500/3816 [==>...........................] - ETA: 1:04:27 Epoch 1 Batch 500 Loss 1.4024 \n",
      " 600/3816 [===>..........................] - ETA: 1:00:56 Epoch 1 Batch 600 Loss 1.3264 \n",
      " 700/3816 [====>.........................] - ETA: 57:57 Epoch 1 Batch 700 Loss 1.3130 \n",
      " 800/3816 [=====>........................] - ETA: 55:15 Epoch 1 Batch 800 Loss 1.3539 \n",
      " 900/3816 [======>.......................] - ETA: 52:49 Epoch 1 Batch 900 Loss 1.3557 \n",
      "1000/3816 [======>.......................] - ETA: 50:32 Epoch 1 Batch 1000 Loss 1.4586 \n",
      "1100/3816 [=======>......................] - ETA: 48:24 Epoch 1 Batch 1100 Loss 1.6278 \n",
      "1200/3816 [========>.....................] - ETA: 46:21 Epoch 1 Batch 1200 Loss 1.3486 \n",
      "1300/3816 [=========>....................] - ETA: 44:21 Epoch 1 Batch 1300 Loss 1.5626 \n",
      "1400/3816 [==========>...................] - ETA: 42:21 Epoch 1 Batch 1400 Loss 1.1661 \n",
      "1500/3816 [==========>...................] - ETA: 40:26 Epoch 1 Batch 1500 Loss 1.4346 \n",
      "1600/3816 [===========>..................] - ETA: 38:40 Epoch 1 Batch 1600 Loss 1.4016 \n",
      "1700/3816 [============>.................] - ETA: 36:50 Epoch 1 Batch 1700 Loss 1.3400 \n",
      "1800/3816 [=============>................] - ETA: 35:00 Epoch 1 Batch 1800 Loss 1.2948 \n",
      "1900/3816 [=============>................] - ETA: 33:12 Epoch 1 Batch 1900 Loss 1.5481 \n",
      "2000/3816 [==============>...............] - ETA: 31:24 Epoch 1 Batch 2000 Loss 1.4198 \n",
      "2100/3816 [===============>..............] - ETA: 29:36 Epoch 1 Batch 2100 Loss 1.5586 \n",
      "2200/3816 [================>.............] - ETA: 27:50 Epoch 1 Batch 2200 Loss 1.4398 \n",
      "2300/3816 [=================>............] - ETA: 26:04 Epoch 1 Batch 2300 Loss 1.4317 \n",
      "2400/3816 [=================>............] - ETA: 24:19 Epoch 1 Batch 2400 Loss 1.5094 \n",
      "2500/3816 [==================>...........] - ETA: 22:35 Epoch 1 Batch 2500 Loss 1.5202 \n",
      "2600/3816 [===================>..........] - ETA: 20:50 Epoch 1 Batch 2600 Loss 1.4196 \n",
      "2700/3816 [====================>.........] - ETA: 19:06 Epoch 1 Batch 2700 Loss 1.2381 \n",
      "2800/3816 [=====================>........] - ETA: 17:22 Epoch 1 Batch 2800 Loss 1.2747 \n",
      "2900/3816 [=====================>........] - ETA: 15:38 Epoch 1 Batch 2900 Loss 1.2659 \n",
      "3000/3816 [======================>.......] - ETA: 13:55 Epoch 1 Batch 3000 Loss 1.3774 \n",
      "3100/3816 [=======================>......] - ETA: 12:12 Epoch 1 Batch 3100 Loss 1.0915 \n",
      "3200/3816 [========================>.....] - ETA: 10:29 Epoch 1 Batch 3200 Loss 1.3416 \n",
      "3300/3816 [========================>.....] - ETA: 8:46 Epoch 1 Batch 3300 Loss 1.4259 \n",
      "3400/3816 [=========================>....] - ETA: 7:04 Epoch 1 Batch 3400 Loss 1.3962 \n",
      "3500/3816 [==========================>...] - ETA: 5:21 Epoch 1 Batch 3500 Loss 1.5317 \n",
      "3600/3816 [===========================>..] - ETA: 3:39 Epoch 1 Batch 3600 Loss 1.5234 \n",
      "3700/3816 [============================>.] - ETA: 1:58 Epoch 1 Batch 3700 Loss 1.4201 \n",
      "3800/3816 [============================>.] - ETA: 16s Epoch 1 Batch 3800 Loss 1.3334 \n",
      "3816/3816 [==============================] - 3936s 1s/step\n",
      "Epoch 1 Loss 0.072328\n",
      "Time taken for 1 epoch 3936.304599046707 sec\n",
      "\n",
      " Epoch 2 Batch 0 Loss 1.6941 \n",
      " 100/3816 [..............................] - ETA: 41:38:41 Epoch 2 Batch 100 Loss 1.6020 \n",
      " 200/3816 [>.............................] - ETA: 20:45:25 Epoch 2 Batch 200 Loss 1.7537 \n",
      " 300/3816 [=>............................] - ETA: 13:46:37 Epoch 2 Batch 300 Loss 1.3132 \n",
      " 400/3816 [==>...........................] - ETA: 10:16:17 Epoch 2 Batch 400 Loss 1.3045 \n",
      " 500/3816 [==>...........................] - ETA: 8:09:25 Epoch 2 Batch 500 Loss 1.5746 \n",
      " 600/3816 [===>..........................] - ETA: 6:44:15 Epoch 2 Batch 600 Loss 1.4992 \n",
      " 700/3816 [====>.........................] - ETA: 5:42:58 Epoch 2 Batch 700 Loss 1.3079 \n",
      " 800/3816 [=====>........................] - ETA: 4:56:37 Epoch 2 Batch 800 Loss 1.6062 \n",
      " 900/3816 [======>.......................] - ETA: 4:20:11 Epoch 2 Batch 900 Loss 1.2240 \n",
      "1000/3816 [======>.......................] - ETA: 3:50:42 Epoch 2 Batch 1000 Loss 1.2387 \n",
      "1100/3816 [=======>......................] - ETA: 3:26:17 Epoch 2 Batch 1100 Loss 1.3951 \n",
      "1200/3816 [========>.....................] - ETA: 3:05:46 Epoch 2 Batch 1200 Loss 1.6169 \n",
      "1300/3816 [=========>....................] - ETA: 2:48:04 Epoch 2 Batch 1300 Loss 1.5524 \n",
      "1400/3816 [==========>...................] - ETA: 2:32:39 Epoch 2 Batch 1400 Loss 1.3114 \n",
      "1500/3816 [==========>...................] - ETA: 2:19:08 Epoch 2 Batch 1500 Loss 1.4694 \n",
      "1600/3816 [===========>..................] - ETA: 2:07:06 Epoch 2 Batch 1600 Loss 1.4886 \n",
      "1700/3816 [============>.................] - ETA: 1:56:16 Epoch 2 Batch 1700 Loss 1.4865 \n",
      "1800/3816 [=============>................] - ETA: 1:46:26 Epoch 2 Batch 1800 Loss 1.3664 \n",
      "1900/3816 [=============>................] - ETA: 1:37:29 Epoch 2 Batch 1900 Loss 1.4280 \n",
      "2000/3816 [==============>...............] - ETA: 1:29:15 Epoch 2 Batch 2000 Loss 1.5309 \n",
      "2100/3816 [===============>..............] - ETA: 1:21:39 Epoch 2 Batch 2100 Loss 1.1949 \n",
      "2200/3816 [================>.............] - ETA: 1:14:35 Epoch 2 Batch 2200 Loss 1.4840 \n",
      "2300/3816 [=================>............] - ETA: 1:08:00 Epoch 2 Batch 2300 Loss 1.1890 \n",
      "2400/3816 [=================>............] - ETA: 1:01:50 Epoch 2 Batch 2400 Loss 1.4663 \n",
      "2500/3816 [==================>...........] - ETA: 56:01 Epoch 2 Batch 2500 Loss 1.3603 \n",
      "2600/3816 [===================>..........] - ETA: 50:32 Epoch 2 Batch 2600 Loss 1.6218 \n",
      "2700/3816 [====================>.........] - ETA: 45:20 Epoch 2 Batch 2700 Loss 1.6703 \n",
      "2800/3816 [=====================>........] - ETA: 40:23 Epoch 2 Batch 2800 Loss 1.4695 \n",
      "2900/3816 [=====================>........] - ETA: 35:40 Epoch 2 Batch 2900 Loss 1.3799 \n",
      "3000/3816 [======================>.......] - ETA: 31:09 Epoch 2 Batch 3000 Loss 1.4145 \n",
      "3100/3816 [=======================>......] - ETA: 26:50 Epoch 2 Batch 3100 Loss 1.5440 \n",
      "3200/3816 [========================>.....] - ETA: 22:40 Epoch 2 Batch 3200 Loss 1.3693 \n",
      "3300/3816 [========================>.....] - ETA: 19:32 Epoch 2 Batch 3300 Loss 1.2426 \n",
      "3400/3816 [=========================>....] - ETA: 15:29 Epoch 2 Batch 3400 Loss 1.2533 \n",
      "3500/3816 [==========================>...] - ETA: 11:34 Epoch 2 Batch 3500 Loss 1.3295 \n",
      "3600/3816 [===========================>..] - ETA: 7:47 Epoch 2 Batch 3600 Loss 1.5227 \n",
      "3700/3816 [============================>.] - ETA: 4:07 Epoch 2 Batch 3700 Loss 1.5741 \n",
      "3800/3816 [============================>.] - ETA: 33s Epoch 2 Batch 3800 Loss 1.3154 \n",
      "3816/3816 [==============================] - 8006s 2s/step\n",
      "Epoch 2 Loss 0.072128\n",
      "Time taken for 1 epoch 4069.626489162445 sec\n",
      "\n",
      " Epoch 3 Batch 0 Loss 1.4868 \n",
      " 100/3816 [..............................] - ETA: 83:39:02 Epoch 3 Batch 100 Loss 1.6073 \n",
      " 200/3816 [>.............................] - ETA: 41:11:27 Epoch 3 Batch 200 Loss 1.4616 \n",
      " 300/3816 [=>............................] - ETA: 27:01:12 Epoch 3 Batch 300 Loss 1.5351 \n",
      " 400/3816 [==>...........................] - ETA: 19:55:19 Epoch 3 Batch 400 Loss 1.5235 \n",
      " 486/3816 [==>...........................] - ETA: 16:08:40"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "progbar = tf.keras.utils.Progbar(len(dataset))\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img, target)\n",
    "        total_loss += t_loss\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            ckpt_manager.save()\n",
    "            print (' Epoch {} Batch {} Loss {:.4f} '.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        progbar.update(batch + 1)\n",
    "        \n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    \n",
    "    ckpt_manager.save()\n",
    "        \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "        \n",
    "    img = tf.expand_dims(load_image_only(image), 0) \n",
    "    \n",
    "    features = encoder(img)\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        \n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Formula:', real_caption)\n",
    "print ('Prediction Formula:', ' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in dataset:\n",
    "    print(img.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
