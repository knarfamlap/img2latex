{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.decoder import Decoder\n",
    "from model.encoder import CNN_Encoder\n",
    "from model.attention import BahdanauAttention\n",
    "from model.decoder import embedding_initializer\n",
    "from components.positional import add_timing_signal_nd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"data/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = list(data_dir.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_paths = []\n",
    "for img in imgs:\n",
    "    imgs_paths.append(os.fspath(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_paths = sorted(set(imgs_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"data/small.formulas.norm.txt\", 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    lines[i] = \"<start> \" + lines[i] + \" <end>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 400\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(lines)\n",
    "train_seqs = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, formula):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img)\n",
    "    return img, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_only(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = img / 255\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets using an 80-20 split\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(imgs_paths,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 80\n",
    "BATCH_SIZE = 2\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train)\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(embedding_dim, 512, vocab_size=top_k+1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img, target):\n",
    "    loss = 0\n",
    "    img = img / 255\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "            \n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "        \n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        return loss, total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restore to latest cehckpoint\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.3033\n",
      "Epoch 1 Loss 0.791744\n",
      "Time taken for 1 epoch 0.5043120384216309 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5353\n",
      "Epoch 2 Loss 0.675387\n",
      "Time taken for 1 epoch 0.21209716796875 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2836\n",
      "Epoch 3 Loss 0.640782\n",
      "Time taken for 1 epoch 0.20547890663146973 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3985\n",
      "Epoch 4 Loss 0.695308\n",
      "Time taken for 1 epoch 0.20968198776245117 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4979\n",
      "Epoch 5 Loss 0.663765\n",
      "Time taken for 1 epoch 0.2128913402557373 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2641\n",
      "Epoch 6 Loss 0.669790\n",
      "Time taken for 1 epoch 0.508753776550293 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0200\n",
      "Epoch 7 Loss 0.697713\n",
      "Time taken for 1 epoch 0.20937514305114746 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2936\n",
      "Epoch 8 Loss 0.693663\n",
      "Time taken for 1 epoch 0.21286988258361816 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1910\n",
      "Epoch 9 Loss 0.653291\n",
      "Time taken for 1 epoch 0.21329307556152344 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9299\n",
      "Epoch 10 Loss 0.677695\n",
      "Time taken for 1 epoch 0.21096301078796387 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.2857\n",
      "Epoch 11 Loss 0.643593\n",
      "Time taken for 1 epoch 0.5432870388031006 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.2915\n",
      "Epoch 12 Loss 0.623258\n",
      "Time taken for 1 epoch 0.2100989818572998 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.6363\n",
      "Epoch 13 Loss 0.640900\n",
      "Time taken for 1 epoch 0.22268414497375488 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.3198\n",
      "Epoch 14 Loss 0.673037\n",
      "Time taken for 1 epoch 0.21956777572631836 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.4677\n",
      "Epoch 15 Loss 0.646784\n",
      "Time taken for 1 epoch 0.21735811233520508 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.4087\n",
      "Epoch 16 Loss 0.644173\n",
      "Time taken for 1 epoch 0.490283727645874 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.0736\n",
      "Epoch 17 Loss 0.621906\n",
      "Time taken for 1 epoch 0.20744013786315918 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.4602\n",
      "Epoch 18 Loss 0.635784\n",
      "Time taken for 1 epoch 0.21156620979309082 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.1534\n",
      "Epoch 19 Loss 0.624926\n",
      "Time taken for 1 epoch 0.22354578971862793 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.2862\n",
      "Epoch 20 Loss 0.634953\n",
      "Time taken for 1 epoch 0.2212071418762207 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img, target)\n",
    "        total_loss += t_loss\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "        \n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "        \n",
    "    img = load_image_only(image)\n",
    "    \n",
    "    features = encoder(img)\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, featues, hidden)\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        \n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    \n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    \n",
    "    return result, attention_plot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    len_result = len(result)\n",
    "    \n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_tittle(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent()) \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 120, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image_only(img_name_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "input must be 4-dimensional[50,120,3] [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f1078e4e36b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_name_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreal_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcap_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Real Formula:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-56c78a735fdc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/model/encoder.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     name=None):\n\u001b[0;32m-> 1011\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1012\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2583\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       return conv2d_eager_fallback(\n\u001b[0m\u001b[1;32m    943\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1029\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0;32m-> 1031\u001b[0;31m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m   1032\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/img2latex/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be 4-dimensional[50,120,3] [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Formula:', real_caption)\n",
    "print ('Prediction Formula:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
